# ABTest 系统设计

![](https://gitee.com/lidaming/assets/raw/master/abtest/abtest_compare.jfif)

在互联网公司的业务发展过程中，用户增长是永恒的主题，因为没有增长也就没有发展，所以在业务发展的早期产品迭代速度往往是越快越好，总之一句话：“怎么快怎么来”，至于系统建设得是否可以满足未来几年的扩展什么的，往往在用户增长面前都会显得很扯淡，因为慢了可能就死掉了。

而当业务发展到一定阶段后，野蛮生长的红利逐渐消退，用户增长空间在可见策略下变得不那么明显的情况下，如何合理地规划产品迭代策略就显得尤为重要了，而具体如何判断产品策略是否有效，往往就需要数据进行判断，其结果决定了该产品或策略的生命力以及与之配套的各类资源的调配，毕竟大部分公司是不会愿意将资源浪费在无效的产品和策略上的。那么，通过什么样的工具或手段才能确保数据驱动策略的有效落地和实施呢？

目前硅谷的很多公司都在通过ABTest及建设与之相适配的实验基础设施平台来实现这样的目标，而国内有条件的互联网公司也在逐步进行尝试。小码农目前所在的公司也在进行这样的尝试，作为比较前沿的一种思路与实践，虽然目前在国内的普及程度还并不是很高，但是作者相信随着国内互联网ToC流量红利的消退，这种工具会被越来越多的公司重视起来。本文将从基础理论及系统设计的角度与大家进行一些初步的探讨。

核心要解决的问题是：

- 如何合理地规划产品迭代策略？

- 如何判断产品策略是否有效？

- 通过什么样的工具或手段为产品策略提供有效的数据支撑？

  

## 基础背景

在具体的业务中同一个产品同一个场景可以有不同的决策。所以决策和设计都是为提升转化率而进行的，不以提升客户转化率和客户留存率为目标的设计和规划，都是耍流氓。

- 页面上某个按钮的颜色使用红色还是蓝色？
- 广告位是放在左侧还是右侧？
- 工具条是放在页面顶部还是放在页面底部？
- 以及不同方案的组合比如蓝色按钮放左侧，蓝色按钮放右侧、红色按钮放右侧、红色按钮放左侧？
- 现在用户画像，推荐算法盛行的当下，哪个推荐算法可以使用户转化率更好？

等等，除了上面提到的策略外还有很多方面的策略设计和方案设计。所有的设计都是为了点击率、转化率，在对比过后，从而得出那个方案更符合预期设计目标，并最终将全部流量切换至符合目标的方案。

并且一旦业务进入深度精细化运营阶段，方案会更加复杂，同一个方案下，不同维护的组合出来N多种选择，最后选出页面打开次数、下单成功率等等，收益最高的策略。

在这么多繁杂的策略中，如果**通过硬编码去实施不同策略的验证，事情会变的复杂并且难以推动，整个周期也会拉长**，整个流程下来，每个策略的验证成本太高。

牛叉大佬，牛叉的发展。在高成本运行下，一定有不合理的存在，一定可以找到科学的合理的方案，去解决。

## 重叠实验基础设施

事实上，关于ABTest及数据驱动运营理念是被普遍接受的，而关键问题是在于如何实施与落地，不能落地的理论再好也是口号。Google作为全球互联网巨头，同时也是一家数据驱动的公司，在其产品迭代的方方面面都十分注重实验，几乎任何影响到用户体验的变化、包括可见的变化（如界面的改动），也包括更细节的变化（如不同的机器学习算法的改进）等都会设计不同的实验加以验证。

这方面Google公司走在了前列，而其发表的论文**《Overlapping Experiment Infrastructure:More, Better, Faster Experimentation》**（重叠实验：更多，更好，更快）更成为各大公司建设自身**实验平台**的基础参考。

在该论文中提出的实验基础设施的设计目标是：**更多、更好、更快**。因为在互联网产品迭代过程中，为了更快的进行迭代，产品或运营通常会要求同一个时间做N个实验，并且同一份流量在不同实验之间互不干扰，每个实验都能得到100%的流量。而要实现这些目标，不仅需要能运行更多实验的实验平台，也需要工具和持续的培训来支持更好、更快的实验，从而有效加快产品迭代的正确性与速度。关于以上三点论文中的具体阐述如下：

- **更多**：我们需要系统具备一定的可伸缩性，以便于能够同时运行更多的实验。与此同时，也需要一定的灵活性：不同的实验需要不同的配置和不同的流量来衡量实验在统计意义上的效果显著性。有些实验只需要改变流量的一个子集，比如日本的流量，并且需要适当的调整流量大小；其他实验则可能会改变所有的流量，并在度量上产生较大的变化，因此可以在较少的流量上运行
- **更好**：无效的实验不应该允许在线上流量上运行。有效但糟糕的实验（如，线上Bug或无意中产生的非常糟糕的结果）应该能很快被发现并被禁止运行。标准化的实验指标应该对所有实验都是容易获取的，以便实验的比较是公平的：如两个实验在计算CTR（点击通过率）等指标时，应该使用相同的过滤器来去除机器人产生的流量。
- **更快**：建立一个实验应该既简单又快速，简单到不需要专业的工程师，也不需要编写任何代码就就可以做到。评价指标应该很快就能被统计出来，以便快速评估实验效果。简单的迭代应该可以快速完成。此外，实验平台除了支持实验进行ABTest，还应该支持以易于理解的方式进行**流量灰度**。

那么具体应该如何构建一套实现上述目标的实验基础设施呢？在讨论具体的实施方案之前，我们还需要理解实验平台中几个重要的基础概念：

- **流量**：即用户的访问，也是实验的样本来源。对流量的切分也是实验平台首先需要解决的关键问题之一。
- **参数**：又称作实验变量，一个实验可能有多种策略或者模型需要在线上进行比较，这些模型或者策略就称作实验参数。

为了实现同时做更多的实验以及实现流量复用，需要将实验平台按照分层模型进行设计。关于分层模型有几个比较关键的概念：

- **域（domain）**:是指流量的一个划分，流量进入后首先划分域。
- **层（layer）**:是系统参数的一个子集。
- **实验（experiment）**:是由零个或多个策略参数构成的，被用于改变传入请求处理方式的过程。

在以上三个关键定义中可以**嵌套域和层。域包含层。层包含实验，也可以包含域**。为了更好的理解它们之前的关系，我们可以分别看下其不同的设置关系:

![](domain-layer-base.jpg)

 a-三层的基本重叠设置

在上图中，我们简单地将参数划分为3个层。在这种情况下，每个请求将最多同时出现在三个实验中，每个实验都只能使用对应层的参数。

![](repeat-norepeat.jpg)

b-具有非重叠和重叠域的设置

在图b中，将流量划分为两个域，一个域是可以具有单个层的**非重叠域**，另一个域可以是具有三个层的**重叠域。**在这种情况下，每个请求首先被分配到非重叠**或**重叠域。如果请求在非重叠域中，那么请求最多在一个实验中，并且可以更改整个参数空间中的任何参数。如果请求在重叠域中，那么请求最多在三个实验中，每个实验只能使用对应层的参数。

虽然b这种嵌套方式看起来略微有点复杂，但它有几个优点。首先，拥有一个不重叠的域，可以允许我们进行一些需要改变大量通常不会同时使用的参数的实验。其次，这种嵌套可以允许我们进行不同的参数划分，例如，我们可以设想三个域：一个非重叠的，一个与参数分区重叠的，一个与参数分区不同的重叠域。最后，嵌套可以让我们更有效地使用空间，这取决于哪些分区最常用，以及哪些跨层参数实验最常用。

![](repeat-norepeat-lanch.jpg)

c-具有非重叠、重叠和启动域的设置

![](multi-complex.jpg)

d-具有多个域的复杂设置

在图c/d中，展示了另外一个概念-**启动层**。启动层与前面讨论的**实验层**在几个关键方面有所不同：

- 启动层总是包含在默认域中（运行所有的流量）。
- 启动层是参数的独立分区，在域中，一个参数可以同时位于最多一个启动层和最多一个“正常”层。
- 为了使启动层与普通实验层之间的参数重叠工作，启动层中的实验有略微不同的语义。具体来说，启动层中的实验为参数提供了一个替代的默认值。换句话说，如果正常实验层中没有实验覆盖一个参数，**那么在启动层实验中，使用指定的替代默认值，启动层实验的行为就像正常实验一样**。相反，如果正常实验层中的实验确实覆盖了这个参数，那么这个实验就会覆盖参数的默认值，不管这个值是指定为系统默认值还是在启动层实验中指定的。

以这种方式定义启动层，可以允许我们在不干扰现有实验的情况下，逐步向所有用户推出更新，并以标准化的方式进行跟踪。启动层的一般用法是为每个启动特性创建一个新的启动层，并在功能完全展开时删除该层(并将新的参数值展开为默认值)。最后，因为启动层的实验通常比较大，所以它们可以用来测试特性之间的交互。

看到这里，大家心中是不是还充满了很多疑问呢？实际上，以上理论有点类似于我们在编程中学习的设计模式，是对经验和实践的总结和抽象，而至于采用什么样的设计模式，就取决于我们系统的复杂度和希望能够支持扩展的程度。在实际设计实验平台时，我们可以根据当前阶段业务的实际需要来进行分层模型的选择，下面我们就以图(b)具有非重叠和重叠域的设置为例，进行一些系统设计方面的讨论。





参考：

https://zhuanlan.zhihu.com/p/46837312